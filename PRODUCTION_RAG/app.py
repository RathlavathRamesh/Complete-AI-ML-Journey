import streamlit as st
from app_core import answer_question

# ======================================================
# PAGE CONFIG
# ======================================================
st.set_page_config(
    page_title="Enterprise Policy RAG Assistant",
    page_icon="ğŸ“˜",
    layout="wide"
)

# ======================================================
# HEADER
# ======================================================
st.markdown(
    """
    <h1 style='text-align:center;'>ğŸ“˜ Enterprise Policy RAG Assistant</h1>
    <p style='text-align:center; color:gray; font-size:16px;'>
    An explainable, enterprise-grade Retrieval-Augmented Generation (RAG) system for policy, HR, and compliance.
    </p>
    """,
    unsafe_allow_html=True
)

st.divider()

# ======================================================
# SIDEBAR
# ======================================================
with st.sidebar:
    st.markdown("## ğŸ§  System Overview")

    st.markdown(
        """
        This application demonstrates a **production-style RAG pipeline**:

        - ğŸ” **Dense Retrieval** (FAISS)
        - ğŸ¯ **Cross-Encoder Re-Ranking**
        - ğŸ§  **Grounded LLM Generation**
        - ğŸ§ª **Faithfulness Evaluation**
        - ğŸ“Š **Transparent Metrics & Insights**

        Designed for **enterprise policy decision-support**, not just chat.
        """
    )

    st.divider()

    st.markdown("### âš™ï¸ Display Controls")
    show_sources = st.checkbox("Show Evidence & Citations", True)
    show_metrics = st.checkbox("Show Raw Metrics (JSON)", False)

# ======================================================
# INPUT
# ======================================================
query = st.chat_input("Ask a policy-related questionâ€¦")

if query:
    with st.spinner("ğŸ” Retrieving evidence and generating a grounded answerâ€¦"):
        data = answer_question(query)

    answer = data["answer"]
    sources = data["sources"]
    metrics = data["metrics"]
    insight = data["insight"]

    # ======================================================
    # TOP KPI / TRUST BAR
    # ======================================================
    st.markdown("### ğŸ“Š System Performance Snapshot")

    kpi1, kpi2, kpi3, kpi4 = st.columns(4)
    kpi1.metric("â± Retrieval (ms)", metrics["retrieval_time_ms"])
    kpi2.metric("ğŸ¯ Re-rank (ms)", metrics["rerank_time_ms"])
    kpi3.metric("ğŸ§  Generation (ms)", metrics["generation_time_ms"])
    kpi4.metric("ğŸ§ª Faithfulness", metrics["faithfulness_score"])

    # ------------------------------------------------------
    # TRUST / CONFIDENCE INTERPRETATION
    # ------------------------------------------------------
    st.markdown("### ğŸ”’ Trust & Confidence Assessment")

    if metrics["faithfulness_score"] >= 0.6:
        st.success("ğŸŸ¢ **High Confidence** â€” The answer is strongly supported by retrieved policy content.")
    elif metrics["faithfulness_score"] >= 0.35:
        st.warning("ğŸŸ¡ **Medium Confidence** â€” The answer is mostly grounded, but review sources if critical.")
    else:
        st.error("ğŸ”´ **Low Confidence** â€” The answer may not be fully supported. Manual verification recommended.")

    st.divider()

    # ======================================================
    # ANSWER
    # ======================================================
    st.markdown("## ğŸ§  Answer")
    st.markdown(answer)

    # ======================================================
    # SYSTEM INSIGHTS (LLM-INTERPRETED)
    # ======================================================
    st.markdown("## ğŸ” System Insights (LLM-Interpreted)")
    st.markdown(
        """
        The following insights are generated by an LLM that **interprets system metrics**
        to explain reliability, performance, and suitability for decision-making.
        """
    )
    st.markdown(insight)

    # ======================================================
    # EVIDENCE & METRICS
    # ======================================================
    st.divider()
    left, right = st.columns(2)

    if show_sources:
        with left:
            with st.expander("ğŸ“š Evidence & Citations", expanded=False):
                st.markdown(
                    """
                    Below are the document segments used to generate this answer.
                    This ensures **traceability and auditability**.
                    """
                )
                st.markdown(sources)

    if show_metrics:
        with right:
            with st.expander("ğŸ“Š Raw Evaluation Metrics (Debug / Audit)", expanded=False):
                st.json(metrics)
